# 派遣会社フィルター
機械学習を使ってSESやエンジニア派遣を行っている企業の求人広告をフィルタリングします。

## 使い方
このリポジトリを「git clone」して、type様のIT系求人広告から適当に募集情報ページのURLを用意します。  
ターミナル上でリポジトリのディレクトリまで移動して、  
```
python flg_checker.py 'https://type.jp/job-1/xxxxx_detail/?companyMessage=false&pathway=4'
```
上記を参考にコマンドを打ち込んでください。  
URLをクオーテーションで囲うのを忘れずに。  

実行すると、
```
会社名: XXXX
年収: 240〜360万円
年収捕捉: 月給20万円〜30万円
派遣フラグ: 1
```
といった感じに結果が返却されます。  
派遣フラグが1ならばそういう会社さんで、0なら自社開発を行っている会社さんということになります。  
だいたい80%くらいの確率で当たります。  

###### 動かなかった場合
動かなかった場合は必要なモジュールが入っていないか、そもそもpython3が入ってないケースが考えられます。  
モジュールが足りない場合は各pyファイルのimport先を見て必要なものをpipなりcondaなりで落として頂ければ。  
MeCabはデプロイが少々複雑なので、もし入っていなければ[ここ](https://github.com/tomboy-jp/MeCab_dep)を参考にしてください。  
Python3が入っていない場合は適当にググって環境構築するところから始めましょう。


## 作り方
・Webクローリング & スクレイピング  
・データクレンジング  
・自然言語処理 & MLモデルの構築  

の三本立てでお送りします。  
おまけもあるかも?  

#### ■Webクローリング & スクレイピング
対象pythonファイル: crawling_type.py  

まずはデータ収集です。  
今回はいつもお世話になっている[type](https://type.jp/)様の求人広告を利用させて頂きました。  
Web上から情報を取得することをクローリング。  
取得した情報から必要な部分だけ抜き出すことをスクレイピングといいます。  

1.get_indexという関数でIT系の求人情報をクローリングし、    
2.get_detailという関数でスクレイピングを行っています。  

クローリングにrequestsを、  
スクレイピングにはreとBeautifulSoupを使用しました。  

#### ■データクレンジング
対象pythonファイル: cleansing.py  

pandasを使って不必要な列を削ぎ落としていきます。  
またラベルデータが欠如しているので、  
取得したデータのdispatch_flg欄に手動でデータ入力をしていきます。  
辛いところではありますが教師なしでは精度が出ないのため、  
私自身が教師になってデータを補完していきます。  
とりあえず合計200件のデータを用意しました。  
少し足りてない感は否めませんが、簡単な二項分類ですし、最低限の精度は出るかと。  
非常に地味なフェイズのため、技術的に特筆すべきところはありません。   

#### ■自然言語処理 & MLモデルの構築  
対象pythonファイル: exe_ml.py

作業の最後にして最も華のあるフェイズです。  

1.出来上がったデータのドキュメント部分をMeCabで分かち書きにしてコーパスを作成(owakati)  
2.tf-idfでベクトル化(nlp)。  
3.グリッドサーチでパラメータを探り、最適化されたモデルを構築(ml_exe)。

###### ・分かち書き
「私はVtuberになりたい」を「私 は Vtuber に なり たい」  
のように品詞ごとにバラすことです。  
MeCabという形態素解析器を使って簡単に行えます。  
詳しくは[こちら](https://github.com/tomboy-jp/MeCab_dep)から。

###### ・tf-idf
tfは「terms frequency」の略語でどれだけの頻度で単語が出てきたかを評価します。  
idfは「invese document frecuency」の略語でどれだけの文章で登場したかを逆評価します。  
この二つのロジックを組み合わせたのがtf-idfです。  
つまり特定の文章で沢山出てきた単語には強い重みが与えられ、  
登場の少ない単語やあまりに一般的な単語(今回の場合は「年収」「転職」など)は軽視されます。  
sklearnが提供するTfidfVectorizerはこれらに加えて、  
ワンホットエンコーディング(単語や文節ごとを一つの特徴として扱うこと)を自動で行ってくれたり、  
何品詞までを一つの特徴として扱うかの設定が出来たり(今回は1~7品詞までを対象に設定しています)、  
ストップワード(無視する単語)の設定ができたりします。  
至れり尽くせりです。  

###### ・モデルの選定
結果的にロジスティック回帰を使用しました。  
最初は今流行のXGBoostやDeepLearningでイケイケドンドンしようかとも思ったのですが、  
GXBoostを含む決定木モデルは今回のようにコーパスが少ないときは過学習を起こしやすいために、  
DeepLearningは今回のもう一つの目的を果たせないために、不採用としました(理由は後述します)。  
対してロジスティック回帰は比較的単純な線形モデルゆえに過学習を起こしにくく、  
中身もきっちり見えるので今回の用途にはうってつけあるとの判断です。  

パラメータは基本グリッドサーチ  
(すべてのパラメータの組み合わせで最も精度が出るパターンを抽出する手法です)に一任していますが、  
L1正規化(重要度の低い特徴をドロップする手法)だけは使わないようにしました(こちらも後述します)。  

最終的なスコアとパラメータは以下の通りとなります。  
```
Best Score: 0.787
Test Score: 0.800
Best Parameters:
{'logisticregression__C': 10000,
 'logisticregression__fit_intercept': True,
 'logisticregression__penalty': 'l2',
 'logisticregression__random_state': 0}
```
もっとデータがあって、DeepLearningを使えば恐らく0.9は目指せると思います。  
sklearnを使えばモデルの入れ替え自体はそう難しくありませんが、  
前者はモチベーションブレイカーなので、まあ気が向いたときにでも。

#### ■おまけ
対象pythonファイル: weight_of_words_ranking.py  

```
プロジェクト,9.728777673242941
案件,8.468665248615574
株,6.554591508811359
ます,5.66746407703486
未経験,4.64236526828179
し,4.3805474237348
も,4.192236074872746
エンジニア,3.915318662632371
を 除く,3.8452164455698314
希望,3.7733057417368716
除く,3.7590268425872173
川崎市,3.362387953950279
横浜市 川崎市,3.322560055197111
印刷,3.3054020599293485
て,3.258506356501901
キャリア,3.2382838116126593
な,3.2326256978442385
スキル,3.172424928474536
い ます,3.1074869406107846
い,3.080712593216811
研修,3.075725370761292
金融,3.066323422432494
の プロジェクト,3.045066335992295
常駐,3.0363878038269014
.
.
.
自由,-2.464278812983278
向い て,-2.5121636115585546
向い,-2.5121636115585546
新規,-2.5493468309881893
技術,-2.5877556932077077
自社 開発,-2.605995747991827
提案,-2.691577808820943
マイコン,-2.6971726249915533
自社 サービス,-2.7001705184854936
コンサルタント,-2.7338758403781003
ゲート,-2.786025215462243
国内,-2.8021170590521374
システム の,-2.808839645710968
販売,-2.8742278279417013
受賞,-2.9556238874992316
生産管理,-2.9708138535862583
社会,-3.0718079852900297
自社 で,-3.122537653864612
クラウド,-3.49355245453089
100 自社,-3.731157220486234
開発,-3.793528931120211
に,-4.3090121514171935
自社,-5.315964404630647
google,-5.410429083230298
製品,-5.818014094938781
```

DeepLearningを使わなかったのもL1正規化を行わなかったのもこれが見たかったからです。  
DeapLearningはその性質上、どんな特徴がどんな重みを持っているかがブラックボックスになってしまいます。  
L1正規化を行うと特徴の数が減ってしまうため、全体像を眺めたい派の人間としては少しあっけないものになってしまいます。  
とはいえあまり意味のなさそうな単語もちらほら混じっているのでストップワードに追加するのは精度改善に貢献しそうですね。  

より詳細なデータが気になる方は「coefficients/coefficients.csv」をご覧ください。  

以上です。  
ありがとうございました。
